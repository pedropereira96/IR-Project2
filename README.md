# IR-Project1

## The goal

Create a document indexer using the SPIMI approach and a basic searching mechanism.

## Design

The application was completely developed in Python, using an object-oriented approach, to better guarantee encapsulation and modularity, with type annotations for all class members and methods to improve readability and unit tests for classes. It has the option of indexing with term positions, though this is disabled by default.

The SPIMI indexing limit per block that was implemented is based on number of postings, while the tokenizer implementation performs string preprocessing, stopword removal, word size filtering and stemming, which are all done in a single pass to prevent iterating each document multiple times.

It is prepared to parse Amazon review data files as the collection of documents to index, which follow the structure described on the beginning of this document: https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt. 

All index files are stored uncompressed as TSV files on the `index/data_source_name` subfolder, with the following structure:
* PostingIndex#.tsv - the final index files. Multiple files are created if the SPIMI posting limit per block is reached, where `'#'` is the block number. It contains the term on the first column of each row, and a posting on each subsequent column, as its document ID. When positions are enabled, each posting will be a string containing the document ID followed by the character `':'` and the list of positions on the document separated by `','`.
* TempBlock#.tsv - temporary index blocks used for merging into the final index. Multiple files are created if the SPIMI posting limit per block is reached, where `'#'` is the block number. The structure is the same as for the final index. Though it isn't necessary these blocks are kept after the final index is created so that they can be inspected.
* MasterIndex.tsv - Contains the document frequency and the final index block number where it can be found. The terms are on the first column of each row, followed on each column by its document frequency and the block number of the posting index.
* DocKeys.tsv - contains the correspondence of surrogate keys to natural keys, that is, the keys generated by the program and the original hexadecimal keys from Amazon.

## Prerequisites

The program was developed using Python 3.8, and in addition the PyStemmer module is required (https://github.com/snowballstem/pystemmer), which is a C implementation of snowballstemmer, and was chosen due to its improved performance compared to the default implementation of snowballstemmer.
Install it with the command:
```
pip install PyStemmer
```
## Usage
```
usage: main.py [-h] --data_path path to data file (.gz)) [--nostopwords] [--stopwords (path to stopwords list)]
               [--word_size (integer number] [--no_word_size] [--no_stemmer] [--use_positions] [--max_post MAX_POST]

optional arguments:
  -h, --help            show this help message and exit
  --data_path (path to data file (.gz))
                        Set the path to the data
  --nostopwords         Disable stop words
  --stopwords (path to stopwords list)
                        Set the path to stop words List
  --word_size (integer number)
                        Set the maximum for the word size filter
  --no_word_size        Disable word size filter
  --no_stemmer          Disable stemmer
  --use_positions       Enable positions indexing
  --max_post MAX_POST   Set the maximum postings per block
```

* The data_path option is the path to the Amazon review data file to be indexed.
* The nostopwords option disables stopwords.
* The stopwords option sets the path to the stopwords list, which should be a simple text file with a stop word on each row. The default is the file in `content/stopwords.txt`, which was taken from http://www.search-engines-book.com/data/stopwords.
* The word_size option sets the word size filter value, words with smaller or equal size than the value set here are filtered. The defaut is 3.
* The no_word_size option disables the word size filter.
* The no_stemmer option disables stemming.
* The use_positions option enables/disables term positions on the index, default is off.

After running the program the data file starts to be indexed using the SPIMI approach and the index files are created as described in the Design section. When it is done some statistics on the process are returned and the user is asked to enter the search term, for which the document frequency and final index file block number in which its postings are contained is returned, that is, the `#` in PostingIndex#.tsv, as described previously.

## Example

Using default parameters:
```
python3 src/main.py --data_path (path)
```

With new data file and stopwords disabled:
```
python3 src/main.py --data_path (path) --nostopwords
```

## The results

Four samples were considered which are available at https://s3.amazonaws.com/amazon-reviews-pds/readme.html, with default values for all program parameters.

| Data source | Nr. of indexed documents | Nr. of postings | Vocabulary size | Total indexing time (s) | Total index size on disk (MB) | Nr. of temporary index segments | Time to set up a query searcher (s) |
|:---------------------------------------------------:|:-------:|:-------:|:-------:|:---------:|:---------:|:-:|:------:|
|amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz   |145431   |3854249  |145431   |28.789505  |29.23137   |4  |0.069073|
|amazon_reviews_us_Digital_Music_Purchase_v1_00.tsv.gz|1688884  |33469012 |1688884  |296.486462 |299.375435 |34 |1.112995|
|amazon_reviews_us_Music_v1_00.tsv.gz                 |4751577  |206911234|4751577  |1678.897414|1766.005263|207|3.102635|
|amazon_reviews_us_Books_v1_00.tsv.gz                 |10319090 |367661536|10319090 |2981.774763|3202.133845|368|4.368793|

These results were produced on a machine with the following specifications:
* CPU: AMD Ryzen 5 4600H with Radeon Graphics
* RAM: 8GB
* Storage: SSD - Samsung MZALQ512HALU-000L2

## Authors

Ivo Félix - 109641 [GitHub](https://github.com/IvoFelix)

João Pedro Pereira - 106346 [GitHub](https://github.com/joaopedropereiraPP)
